# 账单去重功能说明

## 功能概述

Mana 现在支持智能去重功能，在上传账单文件时自动检测并过滤重复的交易记录，避免重复导入相同的数据。

## 工作原理

### 去重策略

系统通过以下关键字段生成交易的唯一标识：

1. **交易日期** (`transactionDate`)
2. **交易金额** (`amount`) - 保留两位小数
3. **交易描述** (`description`) - 忽略大小写和前后空格

**示例**：
```
交易 A: 2024-01-01, ¥50.50, "美团外卖"
交易 B: 2024-01-01, ¥50.5, "  美团外卖  "
```
这两条交易会被识别为重复（金额 50.50 = 50.5，描述忽略空格和大小写）。

### 去重流程

```
用户上传文件
  ↓
解析账单数据
  ↓
数据验证和清理
  ↓
【去重检查】
  ├─ 从数据库获取所有历史交易记录
  ├─ 对比新账单与历史记录
  ├─ 识别重复记录
  └─ 过滤重复，保留唯一记录
  ↓
保存唯一记录到数据库
  ↓
返回去重统计信息
```

## 使用场景

### 场景 1：重复上传相同文件

**问题**：用户不小心上传了相同的账单文件

**结果**：
- 系统自动识别所有记录都是重复的
- 返回错误提示："所有记录都是重复的"
- 不会保存任何数据

### 场景 2：部分重复的账单

**问题**：用户上传的账单文件包含部分已存在的交易

**示例**：
- 历史记录：1月1日-1月15日的账单（100条）
- 新上传：1月10日-1月31日的账单（150条）
- 重复期间：1月10日-1月15日（50条）

**结果**：
- 系统识别出 50 条重复记录
- 保存 100 条唯一记录（1月16日-1月31日）
- 返回详细统计信息

### 场景 3：同一批次内的重复

**问题**：上传的文件本身包含重复记录

**结果**：
- 系统会在同一批次内去重
- 只保留第一次出现的记录
- 统计信息中会显示重复数量

## API 响应

### 成功上传（有去重）

```json
{
  "uploadId": "upload-1234567890-abc123",
  "success": true,
  "beancountContent": "...",
  "stats": {
    "totalRecords": 150,      // 文件中的总记录数
    "validRecords": 150,      // 验证通过的记录数
    "invalidRecords": 0,      // 无效记录数
    "uniqueRecords": 100,     // 唯一记录数（已保存）
    "duplicateRecords": 50    // 重复记录数（已过滤）
  },
  "deduplication": {
    "totalNew": 150,
    "uniqueNew": 100,
    "duplicateNew": 50,
    "duplicateRate": 33.33,   // 重复率（百分比）
    "duplicateExamples": [    // 重复记录示例（最多5条）
      {
        "date": "2024-01-10",
        "amount": 50.5,
        "description": "美团外卖"
      },
      // ... 更多示例
    ]
  }
}
```

### 所有记录都重复

```json
{
  "error": "所有记录都是重复的",
  "message": "上传的账单中所有交易记录都已存在，未发现新的交易",
  "deduplication": {
    "totalNew": 100,
    "uniqueNew": 0,
    "duplicateNew": 100,
    "duplicateRate": 100,
    "duplicateExamples": [...]
  }
}
```

## 技术实现

### 核心函数

#### 1. `generateTransactionHash(bill)`

生成交易的唯一标识：

```typescript
// 输入
{
  transactionDate: "2024-01-01",
  amount: 50.5,
  description: "  美团外卖  "
}

// 输出
"2024-01-01|50.50|美团外卖"
```

#### 2. `deduplicateBills(newBills, existingBills)`

对比新账单与历史记录：

```typescript
const result = deduplicateBills(newBills, existingBills);
// 返回
{
  unique: [...],           // 唯一记录
  duplicates: [...],       // 重复记录
  uniqueCount: 100,
  duplicateCount: 50
}
```

#### 3. `getAllTransactions(db)`

从数据库获取所有历史交易记录：

```typescript
const existingTransactions = await getAllTransactions(db);
// 返回所有 uploads 表中的 parsed_data
```

### 性能考虑

- **哈希算法**：使用字符串拼接，O(1) 时间复杂度
- **去重检查**：使用 Set 数据结构，O(n) 时间复杂度
- **内存占用**：历史记录全部加载到内存（适合中小规模数据）

**优化建议**（未来）：
- 对于大量历史记录（>10万条），可以考虑：
  - 只加载最近 N 个月的记录
  - 使用数据库索引进行去重查询
  - 分批处理大文件

## 测试覆盖

### 测试用例（18个）

1. **哈希生成测试**（4个）
   - 相同交易生成相同哈希
   - 不同交易生成不同哈希
   - 忽略描述的大小写和空格
   - 金额保留两位小数

2. **去重逻辑测试**（4个）
   - 正确识别重复记录
   - 处理没有重复的情况
   - 处理所有记录都重复的情况
   - 处理空的历史记录

3. **批次内去重测试**（3个）
   - 移除同一批次内的重复记录
   - 保留所有唯一记录
   - 处理空数组

4. **重复分组测试**（2个）
   - 找到重复记录的分组
   - 返回空 Map 如果没有重复

5. **统计信息测试**（5个）
   - 格式化无重复的统计信息
   - 格式化有重复的统计信息
   - 生成详细的去重报告
   - 限制重复示例数量为5条
   - 处理没有重复的情况

**测试结果**：131 个测试用例全部通过 ✅

## 使用建议

### 最佳实践

1. **定期上传**
   - 建议每月上传一次账单
   - 避免长时间积累后一次性上传

2. **文件命名**
   - 使用有意义的文件名（如：`alipay-2024-01.csv`）
   - 方便后续查找和管理

3. **检查去重统计**
   - 上传后查看去重统计信息
   - 如果重复率过高，检查是否重复上传

4. **保留原始文件**
   - 系统会保存原始文件到 R2
   - 可以随时下载查看

### 注意事项

1. **金额精度**
   - 系统保留两位小数
   - 50.5 和 50.50 会被识别为相同金额

2. **描述匹配**
   - 忽略大小写和前后空格
   - "美团外卖" 和 "  美团外卖  " 会被识别为相同描述
   - 但 "美团外卖" 和 "美团-外卖" 会被识别为不同描述

3. **日期格式**
   - 必须是标准日期格式（YYYY-MM-DD）
   - 不同日期格式可能导致去重失败

## 常见问题

### Q1: 为什么我的账单被识别为重复？

**A**: 系统基于日期、金额、描述三个字段判断重复。如果这三个字段完全相同，就会被识别为重复。

### Q2: 如何查看被过滤的重复记录？

**A**: 上传响应中的 `deduplication.duplicateExamples` 字段包含最多 5 条重复记录示例。

### Q3: 可以关闭去重功能吗？

**A**: 目前去重功能是自动启用的，无法关闭。这是为了保证数据的准确性。

### Q4: 去重会影响上传速度吗？

**A**: 对于中小规模数据（<10万条历史记录），影响很小。系统使用高效的哈希算法，去重检查通常在毫秒级完成。

### Q5: 如果我确实需要重复的记录怎么办？

**A**: 如果确实需要保存重复记录（如退款后重新购买），可以：
1. 修改交易描述，使其不同
2. 或者联系开发者添加"强制上传"选项

## 未来改进

### 计划中的功能

1. **智能去重**
   - 基于更多字段（如商户、订单号）
   - 支持模糊匹配（相似度阈值）

2. **去重报告**
   - 导出详细的去重报告
   - 可视化重复记录分布

3. **手动管理**
   - 查看所有重复记录
   - 手动选择保留或删除

4. **性能优化**
   - 数据库索引优化
   - 分批处理大文件
   - 增量去重（只对比最近记录）

---

**更新时间**: 2026-01-31
**版本**: v1.0.0
